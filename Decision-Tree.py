# -*- coding: utf-8 -*-
"""ESE 589: Project2_Derick.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14zMqWS9mwxmuoW0LFnCkKWZy4tmP-xwJ
"""

# from google.colab import drive
# drive.mount('/content/drive')

# imports
import csv
import math
import time
import tracemalloc
import itertools
from collections import Counter

def csv_to_data(file_name):
    with open(file_name, 'r') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        my_db = list(csv_reader)
    return my_db

def txt_to_data(file_name):
    with open(file_name, 'r') as file:  # open file
        lines = file.readlines()
    my_db = []  # create empty list, db is going to be list of lists
    # Text-preprocessing
    for line in lines:
        newline = " ".join(line.split(','))  # get rid of commas
        newline = " ".join(newline.split())  # get rid of extra spaces
        newline = newline.split(' ')  # have list of TID [0-index] and rest are items
        my_db.append(newline)  # append to my_db
    return my_db
################################################################################
#Decision Tree Class
class decision_tree:
  def __init__(self, data, attList, classLabel=None, isLeaf=False, splitCrit=None,  split_name=None):


    self.data = data # D
    self.attList = attList #list of tuples contains index and features in dataset (index,attribute)
    self.classLabel = classLabel #Classification name of Node
    self.isLeaf = isLeaf #bottom of tree
    self.splitCrit = splitCrit # splitting criterion found by gain, gain ratio, gini index

    self.children = []  #branches of current node
    self.split_name = split_name #name of attribute after parent is split by splitting criterion
    self.att_idx = {att: idx for idx, att in self.attList.items()} #attribute[key] to index[value] dictionary

  def generate_Decision_Tree(self, mode="info_gain"):
    data_transposed = self.transpose_data()
    classes = data_transposed[-1] # list of class labels
    if len(set(classes)) == 1: # checks if all of the tuples have same class
        self.isLeaf = True
        self.classLabel = classes[0]
        return #No more branches
    # return self # return N as a leaf node labeled with the class C
    if len(self.attList) == 1: # if attribute list is empty
        self.isLeaf = True
        self.classLabel = max(classes)
        return #No more branches

    self.splitCrit = self.selection_method(mode) #calculate optimial split_criterion index
    attribute_index = self.att_idx[self.splitCrit]
    branches = self.split_database(attribute_index)
    del self.attList[attribute_index]
    for b in branches:
        new_node = decision_tree(branches[b], self.attList, split_name=b)
        t_data = new_node.transpose_data()
        classes = t_data[-1]
        if not branches[b]:#if branch empty then leaf node
            new_node.isLeaf = True
            new_node.classLabel = max(classes)
            self.children.append(new_node)
        else: #append new_nodes to children in tree and generate decisiontree
            new_node.generate_Decision_Tree()
            self.children.append(new_node)


  def transpose_data(self): # transpose data for ease
    return list(zip(*self.data))


  def split_database(self, idx):
      split_db = {}
      attribute_classes, attribute_data = self.get_attributes(idx)

      for c in attribute_classes:
          for d in self.data:
              if c not in list(split_db.keys()) and c == d[idx]:
                  split_db[c] = [d]
              elif c in list(split_db.keys()) and c == d[idx]:
                  split_db[c].append(d)
      return split_db


  def get_attributes(self, idx):
      t_split_data = self.transpose_data()
      attribute_data = t_split_data[idx]
      attribute_classes = list(set(attribute_data))
      return attribute_classes, attribute_data

  def binary_search(self, data, value): # binary search algorithm to cut down on runtime for big data -Olog(n) time
      start = 0
      end = len(data) - 1
      while start <= end:
          idx = (start + end) // 2
          if float(data[idx]) == float(value):
              return idx
          elif float(data[idx]) > float(value):
              end = idx - 1
          else:
              start = idx + 1
      return

  def best_continuous_split(self, attribute_data, mode='info_gain'):
      t_data = self.transpose_data()
      split_eval = []
      # attribute_classes.sort(reverse=True)
      inc = len(attribute_data)//10
      split_list = attribute_data[::inc][1:]
      for split in split_list: #inc skips for every other item in list
          split_index = self.binary_search(attribute_data,split)
          left_data, right_data = self.data[:split_index], self.data[split_index:]
          left_node, right_node = decision_tree(left_data,self.attList), decision_tree(right_data,self.attList)
          if mode == 'info_gain':
              left_trans, right_trans = left_node.transpose_data(), right_node.transpose_data()
              left_entropy, right_entropy = left_node.entropyD(left_trans), right_node.entropyD(right_trans)
              gain = ((len(left_data)/len(attribute_data))*left_entropy) + ((len(right_data)/len(attribute_data))*right_entropy)
              entropy = self.entropyD(t_data)
              info_gain = entropy - gain
              split_eval.append((info_gain,split_index))
      return max(split_eval, key=lambda tup:(tup[0]))


  def continuous_datasplit(self, threshold=0.5, max_attributes=30):
      for idx in self.attList:
          if self.attList[idx] == self.attList[len(self.attList)-1]: continue #Don't convert class column
          attribute_classes, attribute_data = self.get_attributes(idx)
          all_check_numeric = all(att.replace('.','',1).isdigit() for att in attribute_classes)
          average_unique = len(attribute_classes) / len(attribute_data)
          unique_logic = threshold <= average_unique <= 1.0
          if (unique_logic or (len(attribute_classes) >= max_attributes)) and all_check_numeric:
              self.data.sort(key=lambda x: float(x[idx]))  # sort  dataset
              attribute_classes, attribute_data = self.get_attributes(idx)
              _,split_idx = self.best_continuous_split(attribute_data)
              for d in range(len(attribute_data)):
                  if d < split_idx:
                      self.data[d][idx] = "left"
                  else:
                      self.data[d][idx] = "right"


  def selection_method(self, mode):  # Calculates gain and gini to determine best split-returns bestatt str
    data_transposed = self.transpose_data()
    if mode == "info_gain":
        info_gain = self.information_gain(data_transposed)
        return info_gain
    elif mode == "gain_ratio":
        gainRatio = self.gain_ratio(data_transposed)
        return gainRatio
    elif mode == "gini_index":
        giniBestAtt, giniRatio = self.gini_ratio(data_transposed)
        return giniBestAtt


  def information_gain(self, data): # data is tranposed data
    gain = self.gain_dictionary(data)
    # print(f"Highest gain from {max(gain, key=gain.get)}:{max(gain.values())}")
    return max(gain, key=gain.get)


  def gain_dictionary(self, data):
    gain = {} # find largest Gain
    entropy = self.entropyD(data)
    attList_tuples = list(self.attList.items())
    for i, att in attList_tuples[:-1]: # don't include class labels, i is attIndex
      gain[att] = entropy - self.info_gain_attribute(i, data)
    return gain


  def entropyD(self, data): # find entropy of gain, original information requirement
    entropy = 0.0
    labels = list(data[-1]) # class label attribute values
    m = list(set(data[-1])) # m is distinct class values
    total = float(len(data[-1])) # total number of tuples
    for c in m: # sum of i to m 
      entropy += -((labels.count(c)/total) * math.log(labels.count(c)/total,2)) # log base 2 
    return entropy


  def info_gain_attribute(self, attIndex, data):
    total = float(len(data[-1])) # total number of tuples
    attValues = Counter(data[attIndex]) # get distinct values and count for each attribute
    info = 0.0
    for k, v in attValues.items(): # sum of j to v
        weight = v/total # v is total count of value of attribute
        dist = 0.0 # total info distribution for attribute
        distCount = {} # distribution for each attribute value for each label
        for t in self.data: # using not transposed self.data
          if k == t[attIndex]: # find matching attribute value in tuple
          # find how many class label tuples show up for each attribute value
            distCount[t[-1]] = 1 + distCount.get(t[-1], 0) # [t-1] is class label
        for count in distCount.values(): # EX: count of yes or no tuples
          dist += -((count/v) * math.log(count/v, 2))
        info += weight * dist
    return info


  def gain_ratio(self, data):
      gain = self.gain_dictionary(data)
      grd = {} # gain ratio dictionary 
      splitInfo = {}
      total = float(len(data[-1]))
      attList_tuples = list(self.attList.items())
      for i, att in attList_tuples[:-1]: # sum of j to v
        attValues = Counter(data[i])
        for count in attValues.values():
          splitInfo[att] = -((count/total) * math.log(count/total, 2)) + splitInfo.get(att, 0)
      for k, v in gain.items(): # calculate gain ratio and split info
          grd[k] = v/splitInfo[k]
      # for k, v in grd.items():
        # print(f"{k}:{v}")
      return max(grd, key=grd.get)

  def gini_ratio(self, data):
      sum = 0
      labels = list(data[-1]) # class label attribute values
      m = list(set(data[-1])) # m is distinct class values
      total = float(len(data[-1])) # total number of tuples
      for c in m:
        sum += (labels.count(c) / total) ** 2
      gi = 1 - sum # gini index
      gid = {}
      lowest = float('inf')
      attList_tuples = list(self.attList.items())
      for i, att in attList_tuples[:-1]:  # don't include class labels, i is attIndex # sum of j to v
        attValues = Counter(data[i]) # get distinct values and count for each attribute
        for L in range(1, len(attValues.keys())): # get every possible subset
          for subset in itertools.combinations(attValues, L):
            subTotal = 0.0
            subsetCount = {}
            notsubsetCount = {}
            for t in self.data:
              if t[i] in subset:
                subTotal += 1.0
                subsetCount[t[-1]] = 1 + subsetCount.get(t[-1], 0)
              else:
                notsubsetCount[t[-1]] = 1 + notsubsetCount.get(t[-1], 0)
            weight1 = float(subTotal/total)
            weight2 = float(1.0 - weight1)
            giniD1 = 0.0
            giniD2 = 0.0
            # print(subset)
            for v in subsetCount.values():
              giniD1 += -((float(v)/subTotal) ** 2.0)
            for v in notsubsetCount.values():
              giniD2 += -((float(v)/(total-subTotal)) ** 2.0)
            giniD1 = weight1 * (1.0 + giniD1)
            giniD2 = weight2 * (1.0 + giniD2)
            gid[tuple(subset)] = gi - giniD1 + giniD2
            if gi - giniD1 + giniD2 < lowest:
              lowest = min(lowest, gi - giniD1 + giniD2)
              bestAtt =att
      return bestAtt, min(gid, key=gid.get)


  def display_tree(self, depth=0):
      node_info = "<Name: " + str(self.split_name)+ " | " \
                  "Classification: " + str(self.classLabel) + " | " \
                  "isLeaf: " + str(self.isLeaf) + " | "    \
                  "BestCriteria: " + str(self.splitCrit) + ">"

      ret = ("\t" * depth)+ "Depth "+ str(depth) + " : " + repr(node_info) + "\n"
      for child in self.children:
          ret += child.display_tree(depth+1)
      return ret

if __name__ == '__main__':
    start = time.time()
    tracemalloc.start()
    # file_name = "benchmarks/textbook_example2.csv" #txtbook example
    # file_name = "benchmarks/Raisin_Dataset.csv" #.csv file example
    # file_name = "benchmarks/abalone.data" #.data or .txt file 25 + classes
    # file_name = "benchmarks/breast-cancer-wisconsin.data"
    # file_name = "benchmarks/scale.csv"
    # file_name = "benchmarks/balloon.data"
    # file_name = "benchmarks/car.data"
    # file_name = "benchmarks/SPECT.csv"
    # file_name = "benchmarks/chess.csv"
    # file_name = "benchmarks/agaricus-lepiota.data"
    # file_name = "benchmarks/spambase.csv"
    # file_name = "benchmarks/soybean.csv"
    # file_name = "benchmarks/congressionalvoting.csv"
    # file_name = "benchmarks/tic-tac-toe.data"
    file_name = "benchmarks/nursery.csv"
    name = file_name.split('.')
    file_type = name[len(name) - 1]
    if file_type == 'csv':
        db = csv_to_data(file_name)
    else:
        db = txt_to_data(file_name)
    #print(db) 
    # for i in db:
    #   print(i)
    D = db[1:] # Generate a decision tree from the training tuples of Data partition, D
    # print(D)
    attribute_list = db[0]
    # print(attribute_list)
    label = attribute_list[-1]
    attribute_list = {idx: att for idx, att in enumerate(attribute_list)}
    # del attribute_list[0]
    N = decision_tree(data=D, attList=attribute_list, classLabel=label, split_name='root')
    N.continuous_datasplit()
    N.generate_Decision_Tree(mode="gini_index") # mode1="info_gain", mode2="gain_ratio", mode3="gini_index"


    # print("#########################################################################################")

    print("Display Decision Tree:")
    print("DataSet: " + file_name)
    print(N.display_tree())
    end = time.time()

    print(f"Run Time: {end - start} seconds")
    print(f"Current Memory: {tracemalloc.get_traced_memory()[0]}")
    print(f"Peak Memory: {tracemalloc.get_traced_memory()[1]}")
    tracemalloc.stop()